{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIdhtv0edPEZ"
      },
      "source": [
        "# Assignment 2: Sequence Modeling\n",
        "\n",
        "## Introduction\n",
        "In this assignment, we will explore language model inference and training. You will implement various inference algorithms and train a number of simple LMs, ranging from n-gram models to sequence to sequence with attention models.\n",
        "\n",
        "Learning objectives:\n",
        "* Understand how to implement different LM generation algorithms.\n",
        "* Grasp core properties of the different generation algorithms.\n",
        "* Understand how to implement and train various LMs, ranging from n-gram models to sequence to sequence architectures.\n",
        "* Understand the tradeoffs of various modeling decisions when training an LM.\n",
        "\n",
        "**Notes:**\n",
        "* In your solution, keep all code as-is except where it's explicitly mentioned to implement a function.\n",
        "* Items marked with a star (★) should be answered in markdown text in the notebook. You will include your final notebook as a pdf with your submission.\n",
        "* We will automatically save files with results to the results sub-directory, these will be used for autograding. You should submit the results directory with your assignment on gradescope.\n",
        "\n",
        "**Submission:**\n",
        "\n",
        "You will submit:\n",
        "\n",
        "* A PDF copy of your completed notebook. This should be titled \"HW1.pdf\". This is used to grade your answers to the problems marked with a (★).\n",
        "* All the json files saved inside the `results/` directory. These will be autograded.\n",
        "\n",
        "**Setup:**\n",
        "\n",
        "We reccomend running this notebook in Collab, as many of the sections will run much faster on a GPU. You should upload the notebook to Collab and select T4 as the GPU, by selecting \"Runtime -> Change Runtime Type\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV_JZ9t2dPEa"
      },
      "source": [
        "# 1. Using a Sequence Model (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjVoXWvCuIvD"
      },
      "source": [
        "### 1.0 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hWpsHe72gN7"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade huggingface_hub datasets transformers\n",
        "!pip install --upgrade torch torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6DAXfE1uIUz"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import json\n",
        "\n",
        "# Load tokenizer/model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
        "model.to(\"cuda\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4u9q6ywzRGI"
      },
      "source": [
        "We will use the following function to compute the next token prediction scores for a given prefix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kq5v61ZzO2n"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def get_next_token_scores(prefix: List[int]) -> List[float]:\n",
        "    \"\"\"\n",
        "    Return raw next-token scores (logits) s(* | prefix), where prefix is a list of token ids.\n",
        "    \"\"\"\n",
        "    out = model(input_ids=torch.tensor(prefix).unsqueeze(0).to(model.device))\n",
        "    next_logits = out.logits[:, -1, :].squeeze(0)\n",
        "    return next_logits.detach().cpu().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OajKy3xYxDxG"
      },
      "source": [
        "For the following problems you will need to convert token indicies to string and string to token indicies. You can use the following to do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rFLqhZYxPaq"
      },
      "outputs": [],
      "source": [
        "def str_to_tokens(text: str) -> List[int]:\n",
        "  return tokenizer.encode(text)\n",
        "\n",
        "def tokens_to_str(tokens: List[int]) -> str:\n",
        "  return tokenizer.decode(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhKQcKpgzcVP"
      },
      "source": [
        "#### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYVRFAF2dPEb"
      },
      "source": [
        "We will use a set of paired sentences in Russian and English for this assignment. We will compute and compare the probabilities of sequences from both languages. We will use 60 sentences from each language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tlidyCB0pya"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# # Load using the local script\n",
        "# dataset = load_dataset(\"wmt/wmt19\", 'ru-en', split='validation')\n",
        "# english_sentences, russian_sentences = zip(*[(sentence['en'], sentence['ru']) for sentence in dataset['translation'][:60]])\n",
        "\n",
        "# instead we can load from the pre-downloaded files\n",
        "import json\n",
        "with open(\"english.json\", \"r\") as f:\n",
        "  english_sentences = json.load(f)\n",
        "with open(\"russian.json\", \"r\") as f:\n",
        "  russian_sentences = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrWSkRqAdPEb"
      },
      "outputs": [],
      "source": [
        "english_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQs6aiTkdPEb"
      },
      "outputs": [],
      "source": [
        "russian_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojim1Aeax3Hj"
      },
      "source": [
        "### 1.1 Scoring Sequences (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK9QRLFJx-6N"
      },
      "source": [
        "You will implement a function that computes the probability p(* | prefix) over wordtypes in the vocabulary given scores, with a temperature parameter (default to 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggzbwi3Ux74v"
      },
      "outputs": [],
      "source": [
        "def next_tok_prob(prefix: List[int], temperature: float):\n",
        "  pass # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B9y_B_dyTjo"
      },
      "source": [
        "Now, you will implement a function that uses your p(* | prefix) function. to compute the negative log likelihood of a given sequence using chain rule decomposition of sequence probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTR2XhPtybvz"
      },
      "outputs": [],
      "source": [
        "def sequence_prob(prefix: str, text: str, temperature: float):\n",
        "  pass #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb8zY2q9yn2l"
      },
      "source": [
        "**Report** Compute negative log likelihood for all of the sequences in the dataset.\n",
        "\n",
        "★ The 20 sentences with the highest and lowest total negative log likelihoods across the two datasets combined.\n",
        "\n",
        "★ The 20 sentences with the highest and lowest total negative log likelihoods, normalized by sequence length, across the two datasets combined.\n",
        "\n",
        "★ What properties do high-likelihood sequences have that low-probability ones don’t?\n",
        "\n",
        "★ How does normalizing by sequence length influence this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU7dYyY_dPEb"
      },
      "source": [
        "Now we will implement a function that computes the perplexity of an entire text dataset.\n",
        "\n",
        "**NOTE: this model does not have a BOS token, so we will use the first token in each sequence as the prefix.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GQpxtamdPEb"
      },
      "outputs": [],
      "source": [
        "def dataset_perplexity(sentences: List[str], temperature: float):\n",
        "\tpass #TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5Bo9hq3dPEb"
      },
      "outputs": [],
      "source": [
        "# fill in your answers for the perplexity of the entire datasets here\n",
        "perplexity_en = None\n",
        "perplexity_ru = None\n",
        "\n",
        "with open(\"results/perplexities.json\", \"w\") as f:\n",
        "\tjson.dump({\"en\": perplexity_en, \"ru\": perplexity_ru}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDD8SnCedPEb"
      },
      "source": [
        "### 1.2 Generating Sequences (15 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV28FQEwdPEb"
      },
      "source": [
        "Implement a function that visualizes two statistics of a set of sequences:\n",
        "\n",
        "(a) the distribution of sequence lengths.\n",
        "\n",
        "(b) frequency-rank distributions of the wordtypes in the vocabulary for different sets of sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2HqKbLHdPEb"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def sequence_length_distribution(sentences: List[str]):\n",
        "\t\"\"\"Visualize the distribution of sequence lengths in a set of sentences.\"\"\"\n",
        "\tpass # TODO\n",
        "\n",
        "def frequency_rank_distribution(sentences: List[str]):\n",
        "\t\"\"\"Visualize the frequency-rank distribution of word types (Zipf's law).\"\"\"\n",
        "\tpass # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY8BXBdNdPEb"
      },
      "source": [
        "Now compute these distributions for the english and russian datasets used in 1.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKWO2y4edPEb"
      },
      "outputs": [],
      "source": [
        "en_length_dist = sequence_length_distribution(english_sentences)\n",
        "ru_length_dist = sequence_length_distribution(russian_sentences)\n",
        "\n",
        "en_freq_dist = frequency_rank_distribution(english_sentences)\n",
        "ru_freq_dist = frequency_rank_distribution(russian_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9MC6jBjdPEc"
      },
      "source": [
        "Implement a function that uses ancestral sampling on top of p(* | prefix) to generate sequences. This function should also be able to use the temperature parameter of p(* | prefix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxxqs0XrdPEc"
      },
      "outputs": [],
      "source": [
        "def generate_sequence(prefix: str, temperature: float, max_length: int):\n",
        "\tpass #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-60Ra0OCdPEc"
      },
      "source": [
        "Now use this function to sample 50 texts with max_length of 100 for each temperature value in {0, 0.7, 1, 1.2} using the prefix \"hello\" and compare their vocabulary distributions.\n",
        "\n",
        "★ Why might temperature result in the observed differences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjueoa2BdPEc"
      },
      "outputs": [],
      "source": [
        "vocab_dist_per_temp = {0.0: None, 0.7: None, 1.0: None, 1.2: None}\n",
        "# each value in vocab_dist_per_temp should be a dictionary mapping each word to its frequency count in the generated texts\n",
        "\n",
        "# TODO sample 50 texts for each temperature value in {0, 0.7, 1, 1.2} using the prefix \"hello\" and compare their vocabulary distributions. Why might temperature result in the observed differences?\n",
        "\n",
        "with open(\"results/vocab_distributions_per_temp.json\", \"w\") as f:\n",
        "\tjson.dump(vocab_dist_per_temp, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3MfYb_-dPEc"
      },
      "source": [
        "Implement a function that modifies and renormalizes p(* | prefix) for ϵ-sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1AUuc33dPEc"
      },
      "outputs": [],
      "source": [
        "def generate_sequence_epsilon(prefix: str, temperature: float, epsilon: float, max_length: int):\n",
        "\tpass #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WK81Ve0dPEc"
      },
      "source": [
        "Report Use ancestral sampling with temperature = 1 and ϵ in {0, 0.05, 0.1} to sample 50 texts of max length 100 for each value of ϵ using the prefix \"hello\", and compare their vocabulary distributions.\n",
        "\n",
        "★ Why might ϵ result in the observed differences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YJbbEFPdPEc"
      },
      "outputs": [],
      "source": [
        "vocab_dist_per_epsilon = {0.0: None, 0.05: None, 0.1: None}\n",
        "# each value in vocab_dist_per_epsilon should be a dictionary mapping each word to its frequency count in the generated texts\n",
        "\n",
        "# TODO\n",
        "\n",
        "with open(\"results/vocab_distributions_per_epsilon.json\", \"w\") as f:\n",
        "\tjson.dump(vocab_dist_per_epsilon, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woB0DPa4dPEc"
      },
      "source": [
        "Implement a function that modifies and renormalizes p(* | prefix) for top-k sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uYCyORkdPEc"
      },
      "outputs": [],
      "source": [
        "def generate_sequence_top_k(prefix: str, temperature: float, top_k: float, max_length: int):\n",
        "\tpass #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ3XBoCvdPEc"
      },
      "source": [
        "Report Use ancestral sampling with temperature = 1 and k in {1, 20, 100} to sample 50 texts with max_length 100 tokens for each value of k with the prefix \"hello\", and compare their vocabulary distributions.\n",
        "\n",
        "★ Why might k result in the observed differences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-MNMmAtdPEc"
      },
      "outputs": [],
      "source": [
        "vocab_dist_per_k = {1: None, 20: None, 100: None}\n",
        "# each value in vocab_dist_per_k should be a dictionary mapping each word to its frequency count in the generated texts\n",
        "\n",
        "# TODO\n",
        "\n",
        "with open(\"results/vocab_distributions_per_k.json\", \"w\") as f:\n",
        "\tjson.dump(vocab_dist_per_k, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPK6nFdydPEc"
      },
      "source": [
        "Implement a function that modifies and renormalizes p(* | prefix) for top-p (nucleus) sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc0N83NCdPEc"
      },
      "outputs": [],
      "source": [
        "def generate_sequence_top_p(prefix: str, temperature: float, top_p: float, max_length: int):\n",
        "\tpass #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LwJk-zHdPEc"
      },
      "source": [
        "Report Use ancestral sampling with temperature = 1 and p in {0.5, 0.9, 1.0} to sample 50 texts with max_length 100 for each value of p with the prefix \"hello\", and compare their length and vocabulary distributions.\n",
        "\n",
        "★ Why might p result in the observed differences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aya0yDU5dPEc"
      },
      "outputs": [],
      "source": [
        "vocab_dist_per_p = {0.5: None, 0.9: None, 1.0: None}\n",
        "# each value in vocab_dist_per_p should be a dictionary mapping each word to its frequency count in the generated texts\n",
        "\n",
        "# TODO\n",
        "\n",
        "with open(\"results/vocab_distributions_per_p.json\", \"w\") as f:\n",
        "\tjson.dump(vocab_dist_per_p, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBHI8n4lpKMT"
      },
      "source": [
        "# 2. Implementing sequence models (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-jya161pmTe"
      },
      "source": [
        "### 2.0 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9cjSjblppqT"
      },
      "outputs": [],
      "source": [
        "# This block handles basic setup and data loading using HuggingFace datasets.\n",
        "\n",
        "# imports\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm\n",
        "import random\n",
        "import pdb\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load WikiText2 from HuggingFace\n",
        "print(\"Loading WikiText2 dataset from HuggingFace (Salesforce/wikitext)...\")\n",
        "wikitext2 = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "# Process the data to create token lists\n",
        "def process_wikitext_split(split_data):\n",
        "    \"\"\"Convert HuggingFace dataset split to list of tokens.\"\"\"\n",
        "    tokens = []\n",
        "    for example in split_data:\n",
        "        text = example['text'].strip()\n",
        "        if text:  # Skip empty lines\n",
        "            # Simple whitespace tokenization\n",
        "            tokens.extend(text.split())\n",
        "    return tokens\n",
        "\n",
        "train_text = process_wikitext_split(wikitext2['train'])\n",
        "validation_text = process_wikitext_split(wikitext2['validation'])\n",
        "test_text = process_wikitext_split(wikitext2['test'])\n",
        "\n",
        "# Build vocabulary from training data\n",
        "print(\"Building vocabulary...\")\n",
        "word_counts = Counter(train_text)\n",
        "\n",
        "# Create a vocab class to match torchtext interface\n",
        "class SimpleVocab:\n",
        "    def __init__(self, word_counts, special_tokens=['<unk>', '<eos>']):\n",
        "        self.itos = special_tokens.copy()\n",
        "        self.unk_index = 0  # Index of <unk> token (first special token)\n",
        "\n",
        "        # Build initial stoi mapping\n",
        "        _stoi = {token: idx for idx, token in enumerate(self.itos)}\n",
        "\n",
        "        # Add all words from training data, sorted by frequency (most common first)\n",
        "        for word, count in word_counts.most_common():\n",
        "            if word not in _stoi:\n",
        "                _stoi[word] = len(self.itos)\n",
        "                self.itos.append(word)\n",
        "\n",
        "        # Use defaultdict to return <unk> index for unknown words\n",
        "        # This matches torchtext behavior where unknown words map to <unk>\n",
        "        self.stoi = defaultdict(lambda: self.unk_index, _stoi)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "vocab = SimpleVocab(word_counts)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(\"Data loading complete!\")\n",
        "print(\"Number of words in the vocabulary: {}\".format(vocab_size))\n",
        "print(\"Number of tokens in training set: {}\".format(len(train_text)))\n",
        "print(\"Number of tokens in validation set: {}\".format(len(validation_text)))\n",
        "print(\"Number of tokens in test set: {}\".format(len(test_text)))\n",
        "print(\"Example text: {}\".format(validation_text[:30]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtJeO6LgptSj"
      },
      "source": [
        "We've implemented a unigram model here as a demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYAehtBFpuCu"
      },
      "outputs": [],
      "source": [
        "class UnigramModel:\n",
        "    def __init__(self, train_text):\n",
        "        self.counts = Counter(train_text)\n",
        "        self.total_count = len(train_text)\n",
        "\n",
        "    def probability(self, word):\n",
        "        return self.counts[word] / self.total_count\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"\"\"Return a list of probabilities for each word in the vocabulary.\"\"\"\n",
        "        return [self.probability(word) for word in vocab.itos]\n",
        "\n",
        "    def perplexity(self, full_text):\n",
        "        \"\"\"Return the perplexity of the model on a text as a float.\n",
        "\n",
        "        full_text -- a list of string tokens\n",
        "        \"\"\"\n",
        "        log_probabilities = []\n",
        "        for word in full_text:\n",
        "            # Note that the base of the log doesn't matter\n",
        "            # as long as the log and exp use the same base.\n",
        "            log_probabilities.append(math.log(max(self.probability(word), 1e-8), 2))\n",
        "        return 2 ** -np.mean(log_probabilities)\n",
        "\n",
        "unigram_demonstration_model = UnigramModel(train_text)\n",
        "print('unigram validation perplexity:',\n",
        "      unigram_demonstration_model.perplexity(validation_text))\n",
        "\n",
        "def check_validity(model):\n",
        "    \"\"\"Performs several sanity checks on your model:\n",
        "    1) That next_word_probabilities returns a valid distribution\n",
        "    2) That perplexity matches a perplexity calculated from next_word_probabilities\n",
        "\n",
        "    Although it is possible to calculate perplexity from next_word_probabilities,\n",
        "    it is still good to have a separate more efficient method that only computes\n",
        "    the probabilities of observed words.\n",
        "    \"\"\"\n",
        "\n",
        "    log_probabilities = []\n",
        "    for i in range(10):\n",
        "        prefix = validation_text[:i]\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        assert min(probs) >= 0, \"Negative value in next_word_probabilities\"\n",
        "        assert max(probs) <= 1 + 1e-8, \"Value larger than 1 in next_word_probabilities\"\n",
        "        assert abs(sum(probs)-1) < 1e-4, \"next_word_probabilities do not sum to 1\"\n",
        "\n",
        "        word_id = vocab.stoi[validation_text[i]]\n",
        "        selected_prob = probs[word_id]\n",
        "        log_probabilities.append(math.log(selected_prob))\n",
        "\n",
        "    perplexity = math.exp(-np.mean(log_probabilities))\n",
        "    your_perplexity = model.perplexity(validation_text[:10])\n",
        "    assert abs(perplexity-your_perplexity) < 0.1, \"your perplexity does not \" + \\\n",
        "    \"match the one we calculated from `next_word_probabilities`,\\n\" + \\\n",
        "    \"at least one of `perplexity` or `next_word_probabilities` is incorrect.\\n\" + \\\n",
        "    f\"we calcuated {perplexity} from `next_word_probabilities`,\\n\" + \\\n",
        "    f\"but your perplexity function returned {your_perplexity} (on a small sample).\"\n",
        "\n",
        "\n",
        "check_validity(unigram_demonstration_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVWpp-djpwRu"
      },
      "source": [
        "To generate from the model, we can sample one word at a time conditioning on the words we have generated so far, just like how we did in the previous part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV7cDfgYpygs"
      },
      "outputs": [],
      "source": [
        "# Note: the prefix tokens will be used by our trigram language model\n",
        "def generate_text(model, n=20, prefix=('<eos>', '<eos>')):\n",
        "    prefix = list(prefix)\n",
        "    for _ in range(n):\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        word = random.choices(vocab.itos, probs)[0]\n",
        "        prefix.append(word)\n",
        "    return ' '.join(prefix)\n",
        "\n",
        "print(generate_text(unigram_demonstration_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1GdhU3Fp9wd"
      },
      "source": [
        "You will need to submit some outputs from the models you implement for us to grade. The following function will be used to generate the required output files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mNba0lJp_ta"
      },
      "outputs": [],
      "source": [
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes.txt\n",
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab.txt\n",
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes_short.txt\n",
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab_short.txt\n",
        "\n",
        "def save_truncated_distribution(model, filename, short=True):\n",
        "    \"\"\"Generate a file of truncated distributions.\n",
        "\n",
        "    Probability distributions over the full vocabulary are large,\n",
        "    so we will truncate the distribution to a smaller vocabulary.\n",
        "\n",
        "    Please do not edit this function\n",
        "    \"\"\"\n",
        "    vocab_name = 'eval_output_vocab'\n",
        "    prefixes_name = 'eval_prefixes'\n",
        "\n",
        "    if short:\n",
        "      vocab_name += '_short'\n",
        "      prefixes_name += '_short'\n",
        "\n",
        "    with open('{}.txt'.format(vocab_name), 'r') as eval_vocab_file:\n",
        "        eval_vocab = [w.strip() for w in eval_vocab_file]\n",
        "    eval_vocab_ids = [vocab.stoi[s] for s in eval_vocab]\n",
        "\n",
        "    all_selected_probabilities = []\n",
        "    with open('{}.txt'.format(prefixes_name), 'r') as eval_prefixes_file:\n",
        "        lines = eval_prefixes_file.readlines()\n",
        "        for line in tqdm.tqdm_notebook(lines, leave=False):\n",
        "            prefix = line.strip().split(' ')\n",
        "            probs = model.next_word_probabilities(prefix)\n",
        "            selected_probs = np.array([probs[i] for i in eval_vocab_ids], dtype=np.float32)\n",
        "            all_selected_probabilities.append(selected_probs)\n",
        "\n",
        "    all_selected_probabilities = np.stack(all_selected_probabilities)\n",
        "    np.save(filename, all_selected_probabilities)\n",
        "    print('saved', filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9cgtwktqCOq"
      },
      "outputs": [],
      "source": [
        "save_truncated_distribution(unigram_demonstration_model, 'results/unigram_demonstration_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjR2saLqqJPX"
      },
      "source": [
        "### 2.1 N-gram Model (5 Points)\n",
        "\n",
        "Now it's time to implement an n-gram language model.\n",
        "\n",
        "Because not every n-gram will have been observed in training, use add-alpha smoothing to make sure no output word has probability 0.\n",
        "\n",
        "$$P(w_2|w_1)=\\frac{C(w_1,w_2)+\\alpha}{C(w_1)+\\alpha\\cdot|V|}$$\n",
        "\n",
        "where $|V|$ is the vocab size and $C$ is the count for the given bigram.  An alpha value around `3e-3`  should work.  Later, we'll replace this smoothing with model backoff. One edge case you will need to handle is at the beginning of the text where you don't have `n-1` prior words.  You can handle this however you like as long as you produce a valid probability distribution, but just using a uniform distribution over the vocabulary is reasonable for the purposes of this project.\n",
        "\n",
        "A properly implemented bi-gram model should get a perplexity below 1500 on the validation set. Please note that the trigram model will probably have a very high perplexity at this point, due to sparsity issues. We'll correct this below.\n",
        "\n",
        "**Note**: Do not change the signature of the `next_word_probabilities` and `perplexity` functions.  We will use these as a common interface for all of the different model types.  Make sure these two functions call `n_gram_probability`, because later we are going to override `n_gram_probability` in a subclass.\n",
        "Also, we suggest pre-computing and caching the counts $C$ when you initialize `NGramModel` for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DxSP8-DqMQ1"
      },
      "outputs": [],
      "source": [
        "class NGramModel:\n",
        "    def __init__(self, train_text, n=2, alpha=3e-3):\n",
        "        # get counts and perform any other setup\n",
        "        self.n = n\n",
        "        self.smoothing = alpha\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def n_gram_probability(self, n_gram):\n",
        "        \"\"\"Return the probability of the last word in an n-gram.\n",
        "\n",
        "        n_gram -- a list of string tokens\n",
        "        returns the conditional probability of the last token given the rest.\n",
        "        \"\"\"\n",
        "        assert len(n_gram) == self.n\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"\"\"Return a list of probabilities for each word in the vocabulary.\"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # use your function n_gram_probability\n",
        "        # vocab.itos contains a list of words to return probabilities for\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def perplexity(self, full_text):\n",
        "        \"\"\" full_text is a list of string tokens\n",
        "        return perplexity as a float \"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # use your function n_gram_probability\n",
        "        # This method should differ a bit from the example unigram model because\n",
        "        # the first n-1 words of full_text must be handled as a special case.\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "unigram_model = NGramModel(train_text, 1)\n",
        "check_validity(unigram_model)\n",
        "print('unigram validation perplexity:', unigram_model.perplexity(validation_text)) # this should be the almost the same as our unigram model perplexity above\n",
        "\n",
        "bigram_model = NGramModel(train_text, n=2)\n",
        "check_validity(bigram_model)\n",
        "print('bigram validation perplexity:', bigram_model.perplexity(validation_text))\n",
        "\n",
        "trigram_model = NGramModel(train_text, n=3)\n",
        "check_validity(trigram_model)\n",
        "print('trigram validation perplexity:', trigram_model.perplexity(validation_text)) # this won't do very well...\n",
        "\n",
        "save_truncated_distribution(bigram_model, 'results/bigram_predictions.npy') # this might take a few minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_S5aIH0qPo_"
      },
      "source": [
        "Please download `bigram_predictions.npy` once you finish this section so that you can submit it.\n",
        "\n",
        "In the block below, please report your bigram validation perplexity.  (We will use this to help us calibrate our scoring on the test set.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVvPrBWWqTfo"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Bigram validation perplexity: ***fill in here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qukgjWJHqWMx"
      },
      "source": [
        "We can also generate samples from the model to get an idea of how it is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvylobe7qYUM"
      },
      "outputs": [],
      "source": [
        "print(generate_text(bigram_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsR8_Ch7AXAZ"
      },
      "source": [
        "We now free up some RAM, **it is important to run the cell below, otherwise you will likely run out of RAM in the Colab runtime.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjKt1ncf_ypz"
      },
      "outputs": [],
      "source": [
        "# Free up some RAM.\n",
        "del bigram_model\n",
        "del trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWXNlsEKb3Mz"
      },
      "source": [
        "### 2.2 N-gram Backoff (5 Points)\n",
        "\n",
        "This basic model works okay for bigrams, but a better strategy (especially for higher-order models) is to use backoff.  Implement backoff with absolute discounting:\n",
        "$$\n",
        "\\begin{align}\n",
        "P\\left(w_i|w_{i-n+1}^{i-1}\\right)&=\\frac{max\\left\\{C(w_{i-n+1}^i)-\\delta,0\\right\\}}{\\sum_{w_i} C\\left(w_{i-n+1}^i\\right)} + \\alpha\\left(w_{i-n+1}^{i-1}\\right) P\\left(w_i|w_{i-n+2}^{i-1}\\right) \\\\\n",
        "\\alpha\\left(w_{i-n+1}^{i-1}\\right)&=\\frac{\\delta N_{1+}\\left(w_{i-n+1}^{i-1}\\right)}{{\\sum_{w_i} C\\left(w_{i-n+1}^i\\right)}}\n",
        "\\end{align}\n",
        "$$\n",
        "where $N_{1+}$ is the number of words that appear after the previous $n-1$ words (the number of times the max will select something other than 0 in the first equation).  If $\\sum_{w_i} C(w_{i-n+1}^i)=0$, use the lower order model probability directly (the above equations would have a division by 0). We found a discount $\\delta$ of 0.9 to work well based on validation performance.  A trigram model with this discount value should get a validation perplexity below 700."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV4e4_mEc7VY"
      },
      "outputs": [],
      "source": [
        "class DiscountBackoffModel(NGramModel):\n",
        "    def __init__(self, train_text, lower_order_model, n=2, delta=0.9):\n",
        "        super().__init__(train_text, n=n)\n",
        "        self.lower_order_model = lower_order_model\n",
        "        self.discount = delta\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def n_gram_probability(self, n_gram):\n",
        "        assert len(n_gram) == self.n\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # back off to the lower_order model with n'=n-1 using its n_gram_probability function\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "\n",
        "bigram_backoff_model = DiscountBackoffModel(train_text, unigram_model, 2)\n",
        "trigram_backoff_model = DiscountBackoffModel(train_text, bigram_backoff_model, 3)\n",
        "check_validity(trigram_backoff_model)\n",
        "print('trigram backoff validation perplexity:', trigram_backoff_model.perplexity(validation_text))\n",
        "\n",
        "save_truncated_distribution(trigram_backoff_model, 'results/discount_backoff_predictions.npy') # this might take a few minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVrWYSMsBRSV"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WJe_trXBTjN"
      },
      "outputs": [],
      "source": [
        "# Release models we don't need any more.\n",
        "del unigram_model\n",
        "del bigram_backoff_model\n",
        "del trigram_backoff_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Xwrx8eO8i-"
      },
      "source": [
        "### 2.3 Smoothed N-gram Model (5 Points)\n",
        "\n",
        "Now, implement Kneser-Ney to replace the unigram base model.\n",
        "$$P(w)\\propto |\\{w':c(w',w) > 0\\}|$$\n",
        "A Kneser-Ney trigram model should get a validation perplexity below 675."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g35KkB5ZBUW8"
      },
      "outputs": [],
      "source": [
        "class KneserNeyBaseModel(NGramModel):\n",
        "    def __init__(self, train_text):\n",
        "        super().__init__(train_text, n=1)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def n_gram_probability(self, n_gram):\n",
        "        assert len(n_gram) == 1\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "kn_base = KneserNeyBaseModel(train_text)\n",
        "check_validity(kn_base)\n",
        "bigram_kn_backoff_model = DiscountBackoffModel(train_text, kn_base, 2)\n",
        "trigram_kn_backoff_model = DiscountBackoffModel(train_text, bigram_kn_backoff_model, 3)\n",
        "print('trigram Kneser-Ney backoff validation perplexity:', trigram_kn_backoff_model.perplexity(validation_text))\n",
        "\n",
        "save_truncated_distribution(trigram_kn_backoff_model, 'results/trigram_kn_predictions.npy') # this might take a few minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXcHXAjL43kd"
      },
      "outputs": [],
      "source": [
        "print(generate_text(trigram_kn_backoff_model))\n",
        "print(generate_text(trigram_kn_backoff_model, prefix=['What','about']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPecL2jMXQ3y"
      },
      "source": [
        "Fill in your trigram backoff perplexities with and without Kneser Ney."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIBVAMe0WV_1"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Trigram backoff validation perplexity: ***fill in here***\n",
        "\n",
        "Trigram backoff with Kneser Ney perplexity: ***fill in here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3TFBf1CBiwp"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGprSePzBpLW"
      },
      "outputs": [],
      "source": [
        "# Delete models we don't need.\n",
        "del kn_base\n",
        "del bigram_kn_backoff_model\n",
        "del trigram_kn_backoff_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbGzQwppf9P5"
      },
      "source": [
        "If you want to learn more about n-gram language models and smoothing techniques, checkout the following paper: https://people.eecs.berkeley.edu/~klein/cs294-5/chen_goodman.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Y0S6XbB1iZ"
      },
      "source": [
        "### 2.4 Neural N-gram Model (5 Points)\n",
        "\n",
        "In this section, you will implement a neural version of an n-gram model.  The model will use a simple feedforward neural network that takes the previous `n-1` words and outputs a distribution over the next word.\n",
        "\n",
        "You will use PyTorch to implement the model.  We've provided a little bit of code to help with the data loading using PyTorch's data loaders (https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "A model with the following architecture and hyperparameters should reach a validation perplexity below 550.\n",
        "* embed the words with dimension 128, then flatten into a single embedding for $n-1$ words (with size $(n-1)*128$)\n",
        "* run 2 hidden layers with 1024 hidden units, then project down to size 128 before the final layer (ie. 4 layers total).\n",
        "* use weight tying for the embedding and final linear layer (this made a very large difference in our experiments); you can do this by creating the output layer with `nn.Linear`, then using `F.embedding` with the linear layer's `.weight` to embed the input\n",
        "* rectified linear activation (ReLU) and dropout 0.1 after first 2 hidden layers. **Note: You will likely find a performance drop if you add a nonlinear activation function after the dimension reduction layer.**\n",
        "* train for 10 epochs with the Adam optimizer (should take around 15-20 minutes)\n",
        "* do early stopping based on validation set perplexity (see HW1A)\n",
        "\n",
        "\n",
        "We encourage you to try other architectures and hyperparameters, and you will likely find some that work better than the ones listed above.  A proper implementation with these should be enough to receive full credit on the assignment, though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jokaz820Fk1h"
      },
      "outputs": [],
      "source": [
        "def ids(tokens):\n",
        "    return [vocab.stoi[t] for t in tokens]\n",
        "\n",
        "assert torch.cuda.is_available(), \"no GPU found, in Colab go to 'Edit->Notebook settings' and choose a GPU hardware accelerator\"\n",
        "\n",
        "class NeuralNgramDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_token_ids, n):\n",
        "        self.text_token_ids = text_token_ids\n",
        "        self.n = n\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_token_ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i < self.n-1:\n",
        "            prev_token_ids = [vocab.stoi['<eos>']] * (self.n-i-1) + self.text_token_ids[:i]\n",
        "        else:\n",
        "            prev_token_ids = self.text_token_ids[i-self.n+1:i]\n",
        "\n",
        "        assert len(prev_token_ids) == self.n-1\n",
        "\n",
        "        x = torch.tensor(prev_token_ids)\n",
        "        y = torch.tensor(self.text_token_ids[i])\n",
        "        return x, y\n",
        "\n",
        "class NeuralNGramNetwork(nn.Module):\n",
        "    # a PyTorch Module that holds the neural network for your model\n",
        "\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a tensor of inputs with shape (batch, n-1)\n",
        "        # this function returns a tensor of log probabilities with shape (batch, vocab_size)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "class NeuralNGramModel:\n",
        "    # a class that wraps NeuralNGramNetwork to handle training and evaluation\n",
        "    # it's ok if this doesn't work for unigram modeling\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.network = NeuralNGramNetwork(n).cuda()\n",
        "\n",
        "    def train(self):\n",
        "        dataset = NeuralNgramDataset(ids(train_text), self.n)\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "        # iterating over train_loader with a for loop will return a 2-tuple of batched tensors\n",
        "        # the first tensor will be previous token ids with size (batch, n-1),\n",
        "        # and the second will be the current token id with size (batch, )\n",
        "        # you will need to move these tensors to GPU, e.g. by using the Tensor.cuda() function.\n",
        "\n",
        "        # this will take some time to run; use tqdm.tqdm_notebook to get a progress bar\n",
        "        # (see Project 0 for example)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        # YOUR CODE HERE\n",
        "        # don't forget self.network.eval()\n",
        "        # you will need to convert text_prefix from strings to numbers with the `ids` function\n",
        "        # if your `perplexity` function below is based on a NeuralNgramDataset DataLoader, you will need to use the same strategy for prefixes with less than n-1 tokens to pass the validity check\n",
        "        #   the data loader appends extra \"<eos>\" (end of sentence) tokens to the start of the input so there are always enough to run the network\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def perplexity(self, text):\n",
        "        # you may want to use a DataLoader here with a NeuralNgramDataset\n",
        "        # don't forget self.network.eval()\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "\n",
        "neural_trigram_model = NeuralNGramModel(3)\n",
        "check_validity(neural_trigram_model)\n",
        "neural_trigram_model.train()\n",
        "print('neural trigram validation perplexity:', neural_trigram_model.perplexity(validation_text))\n",
        "\n",
        "save_truncated_distribution(neural_trigram_model, 'results/neural_trigram_predictions.npy', short=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm-xW4FGXYYi"
      },
      "source": [
        "Fill in your neural trigram perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0cX0k2IW88k"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Neural trigram validation perplexity: ***fill in here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t5PCZnkB1r5"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1yH0lGOB1-S"
      },
      "outputs": [],
      "source": [
        "# Delete model we don't need.\n",
        "del neural_trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOp1Gb_0WjlE"
      },
      "source": [
        "### 2.5 LSTM Model (5 Points)\n",
        "\n",
        "For this stage of the project, you will implement an LSTM language model.\n",
        "\n",
        "For recurrent language modeling, the data batching strategy is a bit different from what is used in some other tasks.  Sentences are concatenated together so that one sentence starts right after the other, and an unfinished sentence will be continued in the next batch.  We'll use the `torchtext` library to manage this batching for you.  To properly deal with this input format, you should save the last state of the LSTM from a batch to feed in as the first state of the next batch.  When you save state across different batches, you should call `.detach()` on the state tensors before the next batch to tell PyTorch not to backpropagate gradients through the state into the batch you have already finished (which will cause a runtime error).\n",
        "\n",
        "We expect your model to reach a validation perplexity below 350.  The following architecture and hyperparameters should be sufficient to get there.\n",
        "* 3 LSTM layers with 512 units\n",
        "* dropout of 0.5 after each LSTM layer\n",
        "* instead of projecting directly from the last LSTM output to the vocabulary size for softmax, project down to a smaller size first (e.g. 512->128->vocab_size). **NOTE: You may find that adding nonlinearities between these layers can hurt performance, try without first.**\n",
        "* use the same weights for the embedding layer and the pre-softmax layer; dimension 128\n",
        "* train with Adam (using default learning rates) for at least 20 epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQQkWUnadPEe"
      },
      "outputs": [],
      "source": [
        "# Create datasets for the LSTM model\n",
        "# Since we're using a custom implementation, we need to create a BPTT iterator\n",
        "\n",
        "class LanguageModelingDataset:\n",
        "    \"\"\"Simple dataset wrapper for language modeling.\"\"\"\n",
        "    def __init__(self, text_tokens):\n",
        "        # Convert tokens to indices\n",
        "        self.data = torch.tensor(ids(text_tokens), dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class BPTTIterator:\n",
        "    \"\"\"Custom BPTT (Backpropagation Through Time) iterator.\n",
        "\n",
        "    This mimics the behavior of torchtext.data.BPTTIterator.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, bptt_len, device):\n",
        "        self.data = dataset.data.to(device)\n",
        "        self.batch_size = batch_size\n",
        "        self.bptt_len = bptt_len\n",
        "        self.device = device\n",
        "\n",
        "        # Calculate number of batches\n",
        "        # We divide the data into batch_size chunks\n",
        "        self.n_tokens = (len(self.data) // batch_size) * batch_size\n",
        "        self.data = self.data[:self.n_tokens].view(batch_size, -1).t().contiguous()\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(0, self.data.size(0) - 1, self.bptt_len):\n",
        "            seq_len = min(self.bptt_len, self.data.size(0) - 1 - i)\n",
        "            # Create batch object with .text and .target attributes\n",
        "            batch = type('obj', (object,), {\n",
        "                'text': self.data[i:i+seq_len],\n",
        "                'target': self.data[i+1:i+1+seq_len]\n",
        "            })()\n",
        "            yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.data.size(0) - 1 + self.bptt_len - 1) // self.bptt_len\n",
        "\n",
        "# Create a namespace to mimic torchtext.data\n",
        "class TorchtextData:\n",
        "    BPTTIterator = BPTTIterator\n",
        "\n",
        "class TorchtextNamespace:\n",
        "    data = TorchtextData()\n",
        "\n",
        "torchtext = TorchtextNamespace()\n",
        "\n",
        "# Create datasets from the text data\n",
        "print(\"Creating LSTM datasets...\")\n",
        "train_dataset = LanguageModelingDataset(train_text)\n",
        "validation_dataset = LanguageModelingDataset(validation_text)\n",
        "test_dataset = LanguageModelingDataset(test_text)\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qOLXKKoc7If"
      },
      "outputs": [],
      "source": [
        "class LSTMNetwork(nn.Module):\n",
        "    # a PyTorch Module that holds the neural network for your model\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        \"\"\"Compute the output of the network.\n",
        "\n",
        "        Note: In the Pytorch LSTM tutorial, the state variable is named \"hidden\":\n",
        "        https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "\n",
        "        The torch.nn.LSTM documentation is quite helpful:\n",
        "        https://pytorch.org/docs/stable/nn.html#lstm\n",
        "\n",
        "        x - a tensor of int64 inputs with shape (seq_len, batch)\n",
        "        state - a tuple of two tensors with shape (num_layers, batch, hidden_size)\n",
        "                representing the hidden state and cell state of the of the LSTM.\n",
        "        returns a tuple with two elements:\n",
        "          - a tensor of log probabilities with shape (seq_len, batch, vocab_size)\n",
        "          - a state tuple returned by applying the LSTM.\n",
        "        \"\"\"\n",
        "\n",
        "        # Note that the nn.LSTM module expects inputs with the sequence\n",
        "        # dimension before the batch by default.\n",
        "        # In this case the dimensions are already in the right order,\n",
        "        # but watch out for this since sometimes people put the batch first\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "class LSTMModel:\n",
        "    \"A class that wraps LSTMNetwork to handle training and evaluation.\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.network = LSTMNetwork().cuda()\n",
        "\n",
        "    def train(self):\n",
        "        train_iterator = torchtext.data.BPTTIterator(train_dataset, batch_size=64,\n",
        "                                                     bptt_len=32, device='cuda')\n",
        "        # Iterate over train_iterator with a for loop to get batches\n",
        "        # each batch object has a .text and .target attribute with\n",
        "        # token id tensors for the input and output respectively.\n",
        "\n",
        "        # The initial state passed into the LSTM should be set to zero.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix):\n",
        "        \"Return a list of probabilities for each word in the vocabulary.\"\n",
        "\n",
        "        prefix_token_tensor = torch.tensor(ids(text_prefix), device='cuda').view(-1, 1)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def dataset_perplexity(self, torchtext_dataset):\n",
        "        \"Return perplexity as a float.\"\n",
        "        # Your code should be very similar to next_word_probabilities, but\n",
        "        # run in a loop over batches. Use torch.no_grad() for extra speed.\n",
        "\n",
        "        iterator = torchtext.data.BPTTIterator(torchtext_dataset, batch_size=64, bptt_len=32, device='cuda')\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "lstm_model = LSTMModel()\n",
        "lstm_model.train()\n",
        "\n",
        "print('lstm validation perplexity:', lstm_model.dataset_perplexity(validation_dataset))\n",
        "save_truncated_distribution(lstm_model, 'results/lstm_predictions.npy', short=False)\n",
        "\n",
        "# Save the LSTM model for use in section 3.1 (text classification with transfer learning)\n",
        "print('\\nSaving LSTM model checkpoint...')\n",
        "torch.save(lstm_model.network.state_dict(), 'lstm_model.pt')\n",
        "print('✓ Saved LSTM model to lstm_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pGhdPQqHx9v"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Fill in your LSTM perplexity.\n",
        "\n",
        "LSTM validation perplexity: ***fill in here***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHpt-nVfdPEe"
      },
      "outputs": [],
      "source": [
        "del lstm_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BokUKhJpN5d"
      },
      "source": [
        "# 3. Acquiring and using word and sentence representations (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHfItAcRdPEe"
      },
      "source": [
        "### 3.0 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyuKwzgOdPEe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import tqdm\n",
        "\n",
        "# Load IMDB dataset\n",
        "print(\"Loading IMDB dataset...\")\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "train_data = imdb_dataset['train']\n",
        "test_data = imdb_dataset['test']\n",
        "\n",
        "print(f\"Train size: {len(train_data)}\")\n",
        "print(f\"Test size: {len(test_data)}\")\n",
        "print(f\"Example: {train_data[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMPmn_lGdPEe"
      },
      "source": [
        "### 3.1 Text classification (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKTCLwAwdPEe"
      },
      "source": [
        "In this section we will use the LSTM model we trained in the previous section, to perform text classification. The hidden state of the LSTM contains rich semantic features about the text it is given. We can use this embedding of the hidden state to provide features for a classifier. Here we will perform sentiment classification, using the LSTM hidden state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvBoJwH5dPEe"
      },
      "source": [
        "First we will load the lstm model trained in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAGbVKyBdPEe"
      },
      "outputs": [],
      "source": [
        "# Load the LSTM language model from section 2 to use as a feature extractor\n",
        "print(\"Loading pre-trained LSTM language model from section 2...\")\n",
        "lstm_feature_extractor = LSTMNetwork()\n",
        "lstm_feature_extractor.load_state_dict(torch.load('lstm_model.pt', map_location='cpu'))\n",
        "lstm_feature_extractor.eval()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "lstm_feature_extractor = lstm_feature_extractor.to(device)\n",
        "print(f\"✓ Successfully loaded LSTM model on {device}\")\n",
        "\n",
        "# Function to convert IMDB text to WikiText2 vocab indices\n",
        "def imdb_text_to_wikitext_tokens(text, wikitext_vocab, max_len=256):\n",
        "    \"\"\"Convert IMDB text to WikiText2 vocabulary token IDs.\"\"\"\n",
        "    # Simple whitespace tokenization (matching WikiText2 preprocessing)\n",
        "    words = text.lower().split()[:max_len]\n",
        "    # Map to WikiText2 vocab (unknown words will map to <unk>)\n",
        "    token_ids = [wikitext_vocab.stoi[word] for word in words]\n",
        "    return token_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ah2fCgLdPEe"
      },
      "source": [
        "Now implement the following function, which should extract features from the LSTM given a list of texts. (HINT: you will have to convert the text into token ids using the same vocab as used to train the LSTM model.)\n",
        "\n",
        "We will compare two different embeddings to use as features for our MLP classifier:\n",
        "\n",
        "1) The mean of the final layer embeddings across all tokens in the sequence.\n",
        "2) The last token final layer embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27YCL-SXdPEe"
      },
      "outputs": [],
      "source": [
        "# Extract LSTM hidden states for all IMDB examples\n",
        "def extract_lstm_features(lstm_model, texts, wikitext_vocab, embedding_type='mean'):\n",
        "    pass # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg5KOCnJdPEf"
      },
      "outputs": [],
      "source": [
        "# Extract features if LSTM model is available\n",
        "# Extract features for train and test sets\n",
        "train_features_mean = extract_lstm_features(\n",
        "\tlstm_feature_extractor,\n",
        "\ttrain_data['text'],\n",
        "\tvocab,  # WikiText2 vocab from section 2\n",
        "\tbatch_size=64,\n",
        "\tembedding_type='mean'\n",
        ")\n",
        "\n",
        "train_features_last = extract_lstm_features(\n",
        "\tlstm_feature_extractor,\n",
        "\ttrain_data['text'],\n",
        "\tvocab,  # WikiText2 vocab from section 2\n",
        "\tbatch_size=64,\n",
        "\tembedding_type='last'\n",
        ")\n",
        "\n",
        "test_features_mean = extract_lstm_features(\n",
        "\tlstm_feature_extractor,\n",
        "\ttest_data['text'],\n",
        "\tvocab,  # WikiText2 vocab from section 2\n",
        "\tbatch_size=64,\n",
        "\tembedding_type='mean'\n",
        ")\n",
        "\n",
        "test_features_last = extract_lstm_features(\n",
        "\tlstm_feature_extractor,\n",
        "\ttest_data['text'],\n",
        "\tvocab,  # WikiText2 vocab from section 2\n",
        "\tbatch_size=64,\n",
        "\tembedding_type='last'\n",
        ")\n",
        "\n",
        "# Save features to disk for reuse\n",
        "torch.save(train_features_mean, 'imdb_train_lstm_features_mean.pt')\n",
        "torch.save(train_features_last, 'imdb_train_lstm_features_last.pt')\n",
        "torch.save(test_features_mean, 'imdb_test_lstm_features_mean.pt')\n",
        "torch.save(test_features_last, 'imdb_test_lstm_features_last.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZEJedLRdPEf"
      },
      "source": [
        "Now we will create dataloaders for each of our feature sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Rsi9HsXdPEf"
      },
      "outputs": [],
      "source": [
        "# Create feature-based dataset\n",
        "class IMDBFeatureDataset(Dataset):\n",
        "    \"\"\"Dataset that uses pre-extracted LSTM features.\"\"\"\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# Create dataloaders with extracted features\n",
        "train_dataset_mean = IMDBFeatureDataset(train_features_mean, train_data['label'])\n",
        "test_dataset_mean = IMDBFeatureDataset(test_features_mean, test_data['label'])\n",
        "\n",
        "train_dataset_last = IMDBFeatureDataset(train_features_last, train_data['label'])\n",
        "test_dataset_last = IMDBFeatureDataset(test_features_last, test_data['label'])\n",
        "\n",
        "train_loader_mean = DataLoader(train_dataset_mean, batch_size=64, shuffle=True)\n",
        "test_loader_mean = DataLoader(test_dataset_mean, batch_size=64, shuffle=False)\n",
        "\n",
        "train_loader_last = DataLoader(train_dataset_last, batch_size=64, shuffle=True)\n",
        "test_loader_last = DataLoader(test_dataset_last, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL9lA8RCdPEf"
      },
      "source": [
        "Using these embeddings, train two MLP classifiers. One on the mean embeddings and one on the last embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8pIB_XEdPEf"
      },
      "outputs": [],
      "source": [
        "# MLP Classifier for LSTM features\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "\t\t# YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def forward(self, x):\n",
        "\t\t# YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "\tdef train(self):\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def class_prediction(self, text_prefix: str) -> int:\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "    def dataset_accuracy(self, dataset) -> float:\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # BEGIN SOLUTION\n",
        "        pass\n",
        "\n",
        "        # END SOLUTION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xszlCnDWdPEf"
      },
      "source": [
        "★ Report the performance of each method using accuracy, precision, recall, and F1 score. Which method performs the best? Which performs the worst? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L_8_i2PdPEf"
      },
      "outputs": [],
      "source": [
        "last_emb_accuracy = None\n",
        "mean_emb_accuracy = None\n",
        "\n",
        "last_emb_precision = None\n",
        "mean_emb_precision = None\n",
        "\n",
        "last_emb_recall = None\n",
        "mean_emb_recall = None\n",
        "\n",
        "last_emb_f1 = None\n",
        "mean_emb_f1 = None\n",
        "\n",
        "mean_predictions = [] # this should be a list of your 0/1 predictions for each item in the test set\n",
        "last_predictions = []\n",
        "\n",
        "with open('results/imdb_lstm_classifier_results.json', 'w') as f:\n",
        "\tjson.dump({\n",
        "\t\t'last_emb_accuracy': last_emb_accuracy,\n",
        "\t\t'mean_emb_accuracy': mean_emb_accuracy,\n",
        "\t\t'last_emb_precision': last_emb_precision,\n",
        "\t\t'mean_emb_precision': mean_emb_precision,\n",
        "\t\t'last_emb_recall': last_emb_recall,\n",
        "\t\t'mean_emb_recall': mean_emb_recall,\n",
        "\t\t'last_emb_f1': last_emb_f1,\n",
        "\t\t'mean_emb_f1': mean_emb_f1,\n",
        "\t\t'mean_predictions': mean_predictions,\n",
        "\t\t'last_predictions': last_predictions,\n",
        "\t}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiQiPGVldPEf"
      },
      "source": [
        "### 3.2 Seq2seq with RNNs (15 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-DHqyr_dPEf"
      },
      "source": [
        "In this section, we'll build and train a sequence-to-sequence RNN model for neural machine translation.\n",
        "\n",
        "We'll start with a baseline encoder-decoder architecture using bidirectional LSTMs.\n",
        "\n",
        "We'll use the Multi30k German-English translation dataset and evaluate our models using BLEU scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7p6HOqGdPEf"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rhBiZ9fdPEf"
      },
      "source": [
        "First we install and import the required dependencies. These include:\n",
        "* `torch` for modeling and training\n",
        "* `torchtext` for data collection\n",
        "* `sentencepiece` for subword tokenization\n",
        "* `sacrebleu` for BLEU score evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAbhTFkLdPEf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade sacrebleu sentencepiece\n",
        "\n",
        "# Standard library imports\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import pdb\n",
        "\n",
        "# Third party imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sacrebleu\n",
        "import sentencepiece\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm.notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRs7Z6CPdPEf"
      },
      "source": [
        "Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU.\n",
        "We'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfG0j-hrdPEf"
      },
      "outputs": [],
      "source": [
        "assert torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcPChdn9dPEf"
      },
      "source": [
        "#### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkDKDFa8dPEf"
      },
      "source": [
        "The data for this assignment comes from the [Multi30K dataset](https://arxiv.org/abs/1605.00459), which contains English and German captions for images from Flickr. We can download and unpack it using `torchtext`. We use the Multi30K dataset because it is simpler than standard translation benchmark datasets and allows for models to be trained and evaluated in a matter of minutes rather than days.\n",
        "\n",
        "We will be translating from German to English in this assignment, but the same techniques apply equally well to any language pair.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIMGJnBbdPEf"
      },
      "outputs": [],
      "source": [
        "# Download Multi30k dataset directly (avoids torchtext compatibility issues)\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from types import SimpleNamespace\n",
        "\n",
        "def download_and_extract_multi30k():\n",
        "    \"\"\"Download and extract Multi30k dataset.\"\"\"\n",
        "    base_url = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
        "    files = [\n",
        "        \"train.de.gz\", \"train.en.gz\",\n",
        "        \"val.de.gz\", \"val.en.gz\",\n",
        "        \"test_2016_flickr.de.gz\", \"test_2016_flickr.en.gz\"\n",
        "    ]\n",
        "\n",
        "    os.makedirs(\"multi30k\", exist_ok=True)\n",
        "\n",
        "    for filename in files:\n",
        "        filepath = os.path.join(\"multi30k\", filename)\n",
        "        if not os.path.exists(filepath.replace('.gz', '')):\n",
        "            try:\n",
        "                url = base_url + filename\n",
        "                urllib.request.urlretrieve(url, filepath)\n",
        "                # Decompress\n",
        "                import gzip\n",
        "                with gzip.open(filepath, 'rb') as f_in:\n",
        "                    with open(filepath.replace('.gz', ''), 'wb') as f_out:\n",
        "                        f_out.write(f_in.read())\n",
        "                os.remove(filepath)  # Remove compressed file\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading {filename}: {e}\")\n",
        "\n",
        "def load_multi30k_split(split_name):\n",
        "    \"\"\"Load Multi30k split from files.\"\"\"\n",
        "    # Map split names to file prefixes\n",
        "    split_map = {\n",
        "        'train': 'train',\n",
        "        'valid': 'val',\n",
        "        'test': 'test_2016_flickr'\n",
        "    }\n",
        "\n",
        "    prefix = split_map[split_name]\n",
        "    de_file = os.path.join(\"multi30k\", f\"{prefix}.de\")\n",
        "    en_file = os.path.join(\"multi30k\", f\"{prefix}.en\")\n",
        "\n",
        "    with open(de_file, 'r', encoding='utf-8') as f_de, \\\n",
        "         open(en_file, 'r', encoding='utf-8') as f_en:\n",
        "        de_lines = [line.strip() for line in f_de]\n",
        "        en_lines = [line.strip() for line in f_en]\n",
        "\n",
        "    return [SimpleNamespace(src=de, trg=en) for de, en in zip(de_lines, en_lines)]\n",
        "\n",
        "# Download dataset if not already present\n",
        "download_and_extract_multi30k()\n",
        "\n",
        "# Load the splits\n",
        "training_data = load_multi30k_split('train')\n",
        "validation_data = load_multi30k_split('valid')\n",
        "test_data = load_multi30k_split('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytiUxT6ZdPEf"
      },
      "source": [
        "Now that we have the data, let's see how large each split is and look at a few examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG7jwoqOdPEf"
      },
      "outputs": [],
      "source": [
        "print(\"Number of training examples:\", len(training_data))\n",
        "print(\"Number of validation examples:\", len(validation_data))\n",
        "print(\"Number of test examples:\", len(test_data))\n",
        "print()\n",
        "\n",
        "for example in training_data[:10]:\n",
        "  print(example.src)\n",
        "  print(example.trg)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUP3c-LHdPEf"
      },
      "source": [
        "#### Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvGQmlRRdPEf"
      },
      "source": [
        "We can use `sentencepiece` to create a joint German-English subword vocabulary from the training corpus. Because the number of training examples is small, we choose a smaller vocabulary size than would be used for large-scale NMT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2h3OEvMdPEf"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    \"pad_id\": 0,\n",
        "    \"bos_id\": 1,\n",
        "    \"eos_id\": 2,\n",
        "    \"unk_id\": 3,\n",
        "    \"input\": \"multi30k/train.de,multi30k/train.en\",\n",
        "    \"vocab_size\": 8000,\n",
        "    \"model_prefix\": \"multi30k\",\n",
        "}\n",
        "combined_args = \" \".join(\n",
        "    \"--{}={}\".format(key, value) for key, value in args.items())\n",
        "sentencepiece.SentencePieceTrainer.Train(combined_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeX8REvhdPEf"
      },
      "source": [
        "This creates two files: `multi30k.model` and `multi30k.vocab`. The first is a binary file containing the relevant data for the vocabulary. The second is a human-readable listing of each subword and its associated score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqT2SVXgdPEf"
      },
      "source": [
        "We can preview the contents of the vocabulary by looking at the first few rows from the human-readable file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaYR9yB0dPEf"
      },
      "outputs": [],
      "source": [
        "!head -n 30 multi30k.vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9YpKDw7dPEf"
      },
      "source": [
        "As we can see, the vocabulary consists of four special tokens (`<pad>` for padding, `<s>` for beginning of sentence (BOS), `</s>` for end of sentence (EOS), `<unk>` for unknown) and a mixture of German and English words and subwords. In order to ensure reversability, word boundaries are encoded with a special unicode character \"▁\" (U+2581)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUy-CHazdPEf"
      },
      "source": [
        "To use the vocabulary, we first need to load it from the binary file produced above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSZmXqdTdPEf"
      },
      "outputs": [],
      "source": [
        "vocab = sentencepiece.SentencePieceProcessor()\n",
        "vocab.Load(\"multi30k.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jetSLyXndPEf"
      },
      "source": [
        "The vocabulary object includes a number of methods for working with full sequences or individual pieces. We explore the most relevant ones below. A complete interface can be found on [GitHub](https://github.com/google/sentencepiece/tree/master/python#usage) for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJxBjIO1dPEf"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary size:\", vocab.GetPieceSize())\n",
        "print()\n",
        "\n",
        "for example in training_data[:3]:\n",
        "  sentence = example.trg\n",
        "  pieces = vocab.EncodeAsPieces(sentence)\n",
        "  indices = vocab.EncodeAsIds(sentence)\n",
        "  print(sentence)\n",
        "  print(pieces)\n",
        "  print(vocab.DecodePieces(pieces))\n",
        "  print(indices)\n",
        "  print(vocab.DecodeIds(indices))\n",
        "  print()\n",
        "\n",
        "piece = vocab.EncodeAsPieces(\"the\")[0]\n",
        "index = vocab.PieceToId(piece)\n",
        "print(piece)\n",
        "print(index)\n",
        "print(vocab.IdToPiece(index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2W89XstdPEg"
      },
      "source": [
        "We define some constants here for the first three special tokens that you may find useful in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysSifciddPEg"
      },
      "outputs": [],
      "source": [
        "pad_id = vocab.PieceToId(\"<pad>\")\n",
        "bos_id = vocab.PieceToId(\"<s>\")\n",
        "eos_id = vocab.PieceToId(\"</s>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh30XH9JdPEg"
      },
      "source": [
        "Note that these tokens will be stripped from the output when converting from word pieces to text. This may be helpful when implementing greedy search and beam search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4C-oFZUdPEg"
      },
      "outputs": [],
      "source": [
        "sentence = training_data[0].trg\n",
        "indices = vocab.EncodeAsIds(sentence)\n",
        "indices_augmented = [bos_id] + indices + [eos_id, pad_id, pad_id, pad_id]\n",
        "print(vocab.DecodeIds(indices))\n",
        "print(vocab.DecodeIds(indices_augmented))\n",
        "print(vocab.DecodeIds(indices) == vocab.DecodeIds(indices_augmented))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO8yV6U7dPEg"
      },
      "source": [
        "Let's begin by defining a batch iterator for the training data. Given a dataset and a batch size, it will iterate over the dataset and yield pairs of tensors containing the subword indices for the source and target sentences in the batch, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BYXnYEIdPEg"
      },
      "outputs": [],
      "source": [
        "def make_batch(sentences):\n",
        "  \"\"\"Convert a list of sentences into a batch of subword indices.\n",
        "\n",
        "  Args:\n",
        "    sentences: A list of sentences, each of which is a string.\n",
        "\n",
        "  Returns:\n",
        "    A LongTensor of size (max_sequence_length, batch_size) containing the\n",
        "    subword indices for the sentences, where max_sequence_length is the length\n",
        "    of the longest sentence as encoded by the subword vocabulary and batch_size\n",
        "    is the number of sentences in the batch. A beginning-of-sentence token\n",
        "    should be included before each sequence, and an end-of-sentence token should\n",
        "    be included after each sequence. Empty slots at the end of shorter sequences\n",
        "    should be filled with padding tokens. The tensor should be located on the\n",
        "    device defined at the beginning of the notebook.\n",
        "  \"\"\"\n",
        "  encoded_sequences = []\n",
        "  for sentence in sentences:\n",
        "    indices = vocab.EncodeAsIds(sentence)\n",
        "    indices_with_special = [bos_id] + indices + [eos_id]\n",
        "    encoded_sequences.append(torch.tensor(indices_with_special, dtype=torch.long, device=device))\n",
        "\n",
        "  # Use pad_sequence to pad them to the same length\n",
        "  # pad_sequence with batch_first=False returns shape (max_len, batch_size)\n",
        "  batch = torch.nn.utils.rnn.pad_sequence(encoded_sequences, batch_first=False, padding_value=pad_id)\n",
        "\n",
        "  return batch\n",
        "\n",
        "def make_batch_iterator(dataset, batch_size, shuffle=False):\n",
        "  \"\"\"Make a batch iterator that yields source-target pairs.\n",
        "\n",
        "  Args:\n",
        "    dataset: A torchtext dataset object.\n",
        "    batch_size: An integer batch size.\n",
        "    shuffle: A boolean indicating whether to shuffle the examples.\n",
        "\n",
        "  Yields:\n",
        "    Pairs of tensors constructed by calling the make_batch function on the\n",
        "    source and target sentences in the current group of examples. The max\n",
        "    sequence length can differ between the source and target tensor, but the\n",
        "    batch size will be the same. The final batch may be smaller than the given\n",
        "    batch size.\n",
        "  \"\"\"\n",
        "\n",
        "  examples = list(dataset)\n",
        "  if shuffle:\n",
        "    random.shuffle(examples)\n",
        "\n",
        "  for start_index in range(0, len(examples), batch_size):\n",
        "    example_batch = examples[start_index:start_index + batch_size]\n",
        "    source_sentences = [example.src for example in example_batch]\n",
        "    target_sentences = [example.trg for example in example_batch]\n",
        "    yield make_batch(source_sentences), make_batch(target_sentences)\n",
        "\n",
        "test_batch = make_batch([\"a test input\", \"a longer input than the first\"])\n",
        "print(\"Example batch tensor:\")\n",
        "print(test_batch)\n",
        "assert test_batch.shape[1] == 2\n",
        "assert test_batch[0, 0] == bos_id\n",
        "assert test_batch[0, 1] == bos_id\n",
        "assert test_batch[-1, 0] == pad_id\n",
        "assert test_batch[-1, 1] == eos_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvLxtVOWdPEg"
      },
      "source": [
        "### Implementing a sequence-to-sequence model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZjvVrhzdPEg"
      },
      "source": [
        "With our data and vocabulary loaded, we're now ready to build a baseline sequence-to-sequence model.  Later on we'll add an attention mechanism to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_GEmmhgdPEg"
      },
      "source": [
        "Below we will define the model. It should consist of a bidirectional LSTM encoder that encodes the input sentence into a fixed-size representation, and an LSTM decoder that uses this representation to produce the output sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5SK-SAkdPEg"
      },
      "outputs": [],
      "source": [
        "class Seq2seqBaseline(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # Initialize your model's parameters here. To get started, we suggest\n",
        "    # setting all embedding and hidden dimensions to 256, using encoder and\n",
        "    # decoder LSTMs with 2 layers, and using a dropout rate of 0.5.\n",
        "\n",
        "    # Implementation tip: To create a bidirectional LSTM, you don't need to\n",
        "    # create two LSTM networks. Instead use nn.LSTM(..., bidirectional=True).\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    ...\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "  def encode(self, source):\n",
        "    \"\"\"Encode the source batch using a bidirectional LSTM encoder.\n",
        "\n",
        "    Args:\n",
        "      source: An integer tensor with shape (max_source_sequence_length,\n",
        "        batch_size) containing subword indices for the source sentences.\n",
        "\n",
        "    Returns:\n",
        "      A tuple with three elements:\n",
        "        encoder_output: The output of the bidirectional LSTM with shape\n",
        "          (max_source_sequence_length, batch_size, 2 * hidden_size).\n",
        "        encoder_mask: A boolean tensor with shape (max_source_sequence_length,\n",
        "          batch_size) indicating which encoder outputs correspond to padding\n",
        "          tokens. Its elements should be True at positions corresponding to\n",
        "          padding tokens and False elsewhere.\n",
        "        encoder_hidden: The final hidden states of the bidirectional LSTM (after\n",
        "          a suitable projection) that will be used to initialize the decoder.\n",
        "          This should be a pair of tensors (h_n, c_n), each with shape\n",
        "          (num_layers, batch_size, hidden_size). Note that the hidden state\n",
        "          returned by the LSTM cannot be used directly. Its initial dimension is\n",
        "          twice the required size because it contains state from two directions.\n",
        "\n",
        "    The first two return values are not required for the baseline model and will\n",
        "    only be used later in the attention model. If desired, they can be replaced\n",
        "    with None for the initial implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementation tip: consider using packed sequences to more easily work\n",
        "    # with the variable-length sequences represented by the source tensor.\n",
        "    # See https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence.\n",
        "\n",
        "    # Implementation tip: there are many simple ways to combine the forward\n",
        "    # and backward portions of the final hidden state, e.g. addition, averaging,\n",
        "    # or a linear transformation of the appropriate size. Any of these\n",
        "    # should let you reach the required performance.\n",
        "\n",
        "    # Compute a tensor containing the length of each source sequence.\n",
        "    lengths = torch.sum(source != pad_id, axis=0)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    ...\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "  def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n",
        "    \"\"\"Run the decoder LSTM starting from an initial hidden state.\n",
        "\n",
        "    The third and fourth arguments are not used in the baseline model, but are\n",
        "    included for compatibility with the attention model in the next section.\n",
        "\n",
        "    Args:\n",
        "      decoder_input: An integer tensor with shape (max_decoder_sequence_length,\n",
        "        batch_size) containing the subword indices for the decoder input. During\n",
        "        evaluation, where decoding proceeds one step at a time, the initial\n",
        "        dimension should be 1.\n",
        "      initial_hidden: A pair of tensors (h_0, c_0) representing the initial\n",
        "        state of the decoder, each with shape (num_layers, batch_size,\n",
        "        hidden_size).\n",
        "      encoder_output: The output of the encoder with shape\n",
        "        (max_source_sequence_length, batch_size, 2 * hidden_size).\n",
        "      encoder_mask: The output mask from the encoder with shape\n",
        "        (max_source_sequence_length, batch_size). Encoder outputs at positions\n",
        "        with a True value correspond to padding tokens and should be ignored.\n",
        "\n",
        "    Returns:\n",
        "      A tuple with three elements:\n",
        "        logits: A tensor with shape (max_decoder_sequence_length, batch_size,\n",
        "          vocab_size) containing unnormalized scores for the next-word\n",
        "          predictions at each position.\n",
        "        decoder_hidden: A pair of tensors (h_n, c_n) with the same shape as\n",
        "          initial_hidden representing the updated decoder state after processing\n",
        "          the decoder input.\n",
        "        attention_weights: This will be implemented later in the attention\n",
        "          model, but in order to maintain compatible type signatures, we also\n",
        "          include it here. This can be None or any other placeholder value.\n",
        "    \"\"\"\n",
        "\n",
        "    # These arguments are not used in the baseline model.\n",
        "    del encoder_output\n",
        "    del encoder_mask\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    ...\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "  def compute_loss(self, source, target):\n",
        "    \"\"\"Run the model on the source and compute the loss on the target.\n",
        "\n",
        "    Args:\n",
        "      source: An integer tensor with shape (max_source_sequence_length,\n",
        "        batch_size) containing subword indices for the source sentences.\n",
        "      target: An integer tensor with shape (max_target_sequence_length,\n",
        "        batch_size) containing subword indices for the target sentences.\n",
        "\n",
        "    Returns:\n",
        "      A scalar float tensor representing cross-entropy loss on the current batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementation tip: don't feed the target tensor directly to the decoder.\n",
        "    # To see why, note that for a target sequence like <s> A B C </s>, you would\n",
        "    # want to run the decoder on the prefix <s> A B C and have it predict the\n",
        "    # suffix A B C </s>.\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    ...\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ6di7fdPEg"
      },
      "source": [
        "We define the following functions for training.  This code will run as provided, but you are welcome to modify the training loop to adjust the optimizer settings, add learning rate decay, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGfxyVbDdPEg"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, batch_size, model_file):\n",
        "  \"\"\"Train the model and save its best checkpoint.\n",
        "\n",
        "  Model performance across epochs is evaluated using token-level accuracy on the\n",
        "  validation set. The best checkpoint obtained during training will be stored on\n",
        "  disk and loaded back into the model at the end of training.\n",
        "  \"\"\"\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  best_accuracy = 0.0\n",
        "  for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
        "    with tqdm.notebook.tqdm(\n",
        "        make_batch_iterator(training_data, batch_size, shuffle=True),\n",
        "        desc=\"epoch {}\".format(epoch + 1),\n",
        "        unit=\"batch\",\n",
        "        total=math.ceil(len(training_data) / batch_size)) as batch_iterator:\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "      for i, (source, target) in enumerate(batch_iterator, start=1):\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.compute_loss(source, target)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_iterator.set_postfix(mean_loss=total_loss / i)\n",
        "      validation_perplexity, validation_accuracy = evaluate_next_token(\n",
        "          model, validation_data)\n",
        "      batch_iterator.set_postfix(\n",
        "          mean_loss=total_loss / i,\n",
        "          validation_perplexity=validation_perplexity,\n",
        "          validation_token_accuracy=validation_accuracy)\n",
        "      if validation_accuracy > best_accuracy:\n",
        "        print(\n",
        "            \"Obtained a new best validation accuracy of {:.2f}, saving model \"\n",
        "            \"checkpoint to {}...\".format(validation_accuracy, model_file))\n",
        "        torch.save(model.state_dict(), model_file)\n",
        "        best_accuracy = validation_accuracy\n",
        "  print(\"Reloading best model checkpoint from {}...\".format(model_file))\n",
        "  model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "def evaluate_next_token(model, dataset, batch_size=64):\n",
        "  \"\"\"Compute token-level perplexity and accuracy metrics.\n",
        "\n",
        "  Note that the perplexity here is over subwords, not words.\n",
        "\n",
        "  This function is used for validation set evaluation at the end of each epoch\n",
        "  and should not be modified.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  total_cross_entropy = 0.0\n",
        "  total_predictions = 0\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for source, target in make_batch_iterator(dataset, batch_size):\n",
        "      encoder_output, encoder_mask, encoder_hidden = model.encode(source)\n",
        "      decoder_input, decoder_target = target[:-1], target[1:]\n",
        "      logits, decoder_hidden, attention_weights = model.decode(\n",
        "          decoder_input, encoder_hidden, encoder_output, encoder_mask)\n",
        "      total_cross_entropy += F.cross_entropy(\n",
        "          logits.permute(1, 2, 0), decoder_target.permute(1, 0),\n",
        "          ignore_index=pad_id, reduction=\"sum\").item()\n",
        "      total_predictions += (decoder_target != pad_id).sum().item()\n",
        "      correct_predictions += (\n",
        "          (decoder_target != pad_id) &\n",
        "          (decoder_target == logits.argmax(2))).sum().item()\n",
        "  perplexity = math.exp(total_cross_entropy / total_predictions)\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  return perplexity, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHBDzTnRdPEg"
      },
      "source": [
        "We can now train the baseline model.\n",
        "\n",
        "Since we haven't yet defined a decoding method to output an entire string, we will measure performance for now by computing perplexity and the accuracy of predicting the next token given a gold prefix of the output. A correct implementation should get a validation token accuracy above 55%. The training code will automatically save the model with the highest validation accuracy and reload that checkpoint's parameters at the end of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6oEAUcpdPEg"
      },
      "outputs": [],
      "source": [
        "# You are welcome to adjust these parameters based on your model implementation.\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "\n",
        "baseline_model = Seq2seqBaseline().to(device)\n",
        "train(baseline_model, num_epochs, batch_size, \"baseline_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfjXPT0_dPEg"
      },
      "source": [
        "**Download your baseline model here.** Once you have a model you are happy with, you are encouraged to download it or save it to your Google Drive in case your session disconnects. The best baseline model has been saved to `baseline_model.pt` in the local filesystem. You will need a trained model while implementing inference below and to generate your final predictions. To download session files from Collab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPIqxURidPEg"
      },
      "source": [
        "For evaluation, we also need to be able to generate entire strings from the model. We'll first define a greedy inference procedure here. Later on, we'll implement beam search.\n",
        "\n",
        "A correct implementation of should get above 20 BLEU on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X58DZTwdPEg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WBj_3T0dPEg"
      },
      "outputs": [],
      "source": [
        "def predict_greedy(model, sentences, max_length=100):\n",
        "  \"\"\"Make predictions for the given inputs using greedy inference.\n",
        "\n",
        "  Args:\n",
        "    model: A sequence-to-sequence model.\n",
        "    sentences: A list of input sentences, represented as strings.\n",
        "    max_length: The maximum length at which to truncate outputs in order to\n",
        "      avoid non-terminating inference.\n",
        "\n",
        "  Returns:\n",
        "    A list of predicted translations, represented as strings.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # Encode all source sentences in batch\n",
        "    source_batch = make_batch(sentences)\n",
        "    encoder_output, encoder_mask, encoder_hidden = model.encode(source_batch)\n",
        "    batch_size = source_batch.shape[1]\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = torch.full((1, batch_size), bos_id, dtype=torch.long, device=device)\n",
        "\n",
        "    generated_tokens = []\n",
        "    finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "    for _ in range(max_length):\n",
        "      logits, decoder_hidden, _ = model.decode(\n",
        "          decoder_input, decoder_hidden, encoder_output, encoder_mask)\n",
        "\n",
        "      # logits shape: (1, batch_size, vocab_size)\n",
        "      logits = logits.squeeze(0)  # Shape: (batch_size, vocab_size)\n",
        "      logits[finished, pad_id] += 1e9\n",
        "      next_tokens = logits.argmax(dim=1)  # Shape: (batch_size,)\n",
        "      generated_tokens.append(next_tokens)\n",
        "\n",
        "      finished = finished | (next_tokens == eos_id)\n",
        "\n",
        "      if finished.all():\n",
        "        break\n",
        "\n",
        "      decoder_input = next_tokens.unsqueeze(0)  # Shape: (1, batch_size)\n",
        "\n",
        "    generated_tokens = torch.stack(generated_tokens)  # Shape: (seq_len, batch_size)\n",
        "    predictions = []\n",
        "    for i in range(batch_size):\n",
        "      tokens = generated_tokens[:, i].tolist()\n",
        "      if eos_id in tokens:\n",
        "        eos_idx = tokens.index(eos_id)\n",
        "        tokens = tokens[:eos_idx]\n",
        "      prediction = vocab.DecodeIds(tokens)\n",
        "      predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "def evaluate(model, dataset, batch_size=64, method=\"greedy\", model_name=\"\"):\n",
        "  assert method in {\"greedy\", \"beam\"}\n",
        "  source_sentences = [example.src for example in dataset]\n",
        "  target_sentences = [example.trg for example in dataset]\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  with torch.no_grad():\n",
        "    for start_index in range(0, len(source_sentences), batch_size):\n",
        "      if method == \"greedy\":\n",
        "        prediction_batch = predict_greedy(\n",
        "            model, source_sentences[start_index:start_index + batch_size])\n",
        "      else:\n",
        "        prediction_batch = predict_beam(\n",
        "            model, source_sentences[start_index:start_index + batch_size])\n",
        "        prediction_batch = [candidates[0] for candidates in prediction_batch]\n",
        "      predictions.extend(prediction_batch)\n",
        "  with open(f\"results/{model_name}_seq2seq_predictions_{method}.json\", \"w\") as f:\n",
        "    json.dump(predictions, f)\n",
        "  return sacrebleu.corpus_bleu(predictions, [target_sentences]).score\n",
        "\n",
        "print(\"Baseline model validation BLEU using greedy search:\",\n",
        "      evaluate(baseline_model, validation_data, model_name=\"baseline\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur9ty2wgdPEg"
      },
      "outputs": [],
      "source": [
        "def show_predictions(model, num_examples=4, include_beam=False):\n",
        "  for example in validation_data[:num_examples]:\n",
        "    print(\"Input:\")\n",
        "    print(\" \", example.src)\n",
        "    print(\"Target:\")\n",
        "    print(\" \", example.trg)\n",
        "    print(\"Greedy prediction:\")\n",
        "    print(\" \", predict_greedy(model, [example.src])[0])\n",
        "    if include_beam:\n",
        "      print(\"Beam predictions:\")\n",
        "      for candidate in predict_beam(model, [example.src])[0]:\n",
        "        print(\" \", candidate)\n",
        "    print()\n",
        "\n",
        "print(\"Baseline model sample predictions:\")\n",
        "print()\n",
        "show_predictions(baseline_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9rwu_gMpSfe"
      },
      "source": [
        "# 4. Attention (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb7NfD4BD7I_"
      },
      "source": [
        "### 4.1 Sequence-to-sequence model with attention (15 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SNJwlAO9N0k"
      },
      "source": [
        "Next, we extend the seq2seq model from the previous part to include an attention mechanism in the decoder. This circumvents the need to store all information about the source sentence in a fixed-size representation, and should substantially improve performance and convergence time.\n",
        "\n",
        "Your implementation should use bilinear attention, where the attention distribution over the encoder outputs $e_1, \\dots, e_n$ given a decoder LSTM output $d$ is obtained via a softmax of the dot products after a suitable projection to get them to the same size: $w_i \\propto \\exp ( d^\\top W e_i )$. The unnormalized attention logits for encoder outputs corresponding to padding tokens should be offset with a large negative value to ensure that the corresponding attention weights are $0$.\n",
        "\n",
        "After computing the attention distribution, take a weighted sum of the projected encoder outputs to obtain the attention context $c = \\sum_i w_i We_i$, and add this to the decoder output $d$ to obtain the final representation to be passed to the vocabulary projection layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-11T17:00:53.912668Z",
          "iopub.status.busy": "2022-02-11T17:00:53.912398Z",
          "iopub.status.idle": "2022-02-11T17:00:53.922526Z",
          "shell.execute_reply": "2022-02-11T17:00:53.921731Z",
          "shell.execute_reply.started": "2022-02-11T17:00:53.912632Z"
        },
        "id": "0uI8j0Sa-gNa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Seq2seqAttention(Seq2seqBaseline):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # Initialize any additional parameters needed for this model that are not\n",
        "    # already included in the baseline model.\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    ...\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "  def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n",
        "    \"\"\"Run the decoder LSTM starting from an initial hidden state.\n",
        "\n",
        "    The third and fourth arguments are not used in the baseline model, but are\n",
        "    included for compatibility with the attention model in the next section.\n",
        "\n",
        "    Args:\n",
        "      decoder_input: An integer tensor with shape (max_decoder_sequence_length,\n",
        "        batch_size) containing the subword indices for the decoder input. During\n",
        "        evaluation, where decoding proceeds one step at a time, the initial\n",
        "        dimension should be 1.\n",
        "      initial_hidden: A pair of tensors (h_0, c_0) representing the initial\n",
        "        state of the decoder, each with shape (num_layers, batch_size,\n",
        "        hidden_size).\n",
        "      encoder_output: The output of the encoder with shape\n",
        "        (max_source_sequence_length, batch_size, 2 * hidden_size).\n",
        "      encoder_mask: The output mask from the encoder with shape\n",
        "        (max_source_sequence_length, batch_size). Encoder outputs at positions\n",
        "        with a True value correspond to padding tokens and should be ignored.\n",
        "\n",
        "    Returns:\n",
        "      A tuple with three elements:\n",
        "        logits: A tensor with shape (max_decoder_sequence_length, batch_size,\n",
        "          vocab_size) containing unnormalized scores for the next-word\n",
        "          predictions at each position.\n",
        "        decoder_hidden: A pair of tensors (h_n, c_n) with the same shape as\n",
        "          initial_hidden representing the updated decoder state after processing\n",
        "          the decoder input.\n",
        "        attention_weights: A tensor with shape (max_decoder_sequence_length,\n",
        "          batch_size, max_source_sequence_length) representing the normalized\n",
        "          attention weights. This should sum to 1 along the last dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementation tip: use a large negative number like -1e9 instead of\n",
        "    # float(\"-inf\") when masking logits to avoid numerical issues.\n",
        "\n",
        "    # Implementation tip: the function torch.einsum may be useful here.\n",
        "    # See https://rockt.github.io/2018/04/30/einsum for a tutorial.\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    ...\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgHYmMDR9lAd"
      },
      "source": [
        "As before, we can train an attention model using the provided training code.\n",
        "\n",
        "A correct implementation should get a validation token accuracy above 64 and a validation BLEU above 36 with greedy search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-11T17:03:49.338267Z",
          "iopub.status.busy": "2022-02-11T17:03:49.337985Z",
          "iopub.status.idle": "2022-02-11T17:10:44.602136Z",
          "shell.execute_reply": "2022-02-11T17:10:44.601381Z",
          "shell.execute_reply.started": "2022-02-11T17:03:49.338236Z"
        },
        "id": "0YvMnDHxJOZv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# You are welcome to adjust these parameters based on your model implementation.\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "\n",
        "attention_model = Seq2seqAttention().to(device)\n",
        "train(attention_model, num_epochs, batch_size, \"attention_model.pt\")\n",
        "print(\"Attention model validation BLEU using greedy search:\",\n",
        "      evaluate(attention_model, validation_data, model_name=\"attention\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey4qZ_U4bcFF"
      },
      "source": [
        "**Download your attention model here.** Once you have a model you are happy with, you are encouraged to download it or save it to your Google Drive in case your session disconnects. The best attention model has been saved to `attention_model.pt` in the local filesystem. You will need a trained model while implementing beam search below and to generate your final predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2dWb-JrA2bR"
      },
      "outputs": [],
      "source": [
        "print(\"Attention model validation BLEU using greedy search:\",\n",
        "      evaluate(attention_model, validation_data, model_name=\"attention\"))\n",
        "print()\n",
        "print(\"Attention model sample predictions:\")\n",
        "print()\n",
        "show_predictions(attention_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2nWxKeEdPEh"
      },
      "source": [
        "### 4.2 Attention visualization: Analysis (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2pPix6pdPEh"
      },
      "source": [
        "Once you have everything working in the section above, add some code here to visualize the decoder attention learned by the attention model using `matplotlib`.\n",
        "\n",
        "You may visualize decoder attention on gold source-target pairs from the validation data. You do not need to run any inference.\n",
        "\n",
        "For this section, let's interpret the attention maps generated by your model. You should include:\n",
        "\n",
        "★ A figure with attention map plots for 4 sentence pairs from the validation set. We encourage you to look through more maps to aid your analysis, but please only include 4 representative plots in the figure.\n",
        "\n",
        "★ A brief discussion over trends you discover in the plots. Do the maps line up with your intuition, are there any surprising alignments? Are there any many-to-one or many-to-many alignments, or mainly one-to-one? Using a tool like Google Translate on substrings may help give some insight into this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aSTeo9EdPEh"
      },
      "outputs": [],
      "source": [
        "# You may find the following annotated heatmap tutorial helpful:\n",
        "# https://matplotlib.org/3.1.3/gallery/images_contours_and_fields/image_annotated_heatmap.html.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "...\n",
        "\n",
        "# BEGIN SOLUTION\n",
        "\n",
        "# END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZgUuUhNdPEh"
      },
      "source": [
        "\\* Describe your findings in this text cell. \\*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u15C5gGLMVSa"
      },
      "source": [
        "# 5. Extra Credit: Beam Search (optional; 10 bonus points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7I01Tl1MYR9"
      },
      "source": [
        "For extra credit, let's implement beam search.\n",
        "\n",
        "Similar to greedy search, beam search generates one token at a time. However, rather than keeping only the single best hypothesis, we instead keep the top $k$ candidates at each time step. This is accomplished by computing the set of next-token extensions for each item on the beam and finding the top $k$ across all candidates according to total log-probability.\n",
        "\n",
        "Candidates that are finished should stay on the beam through the end of inference. The search process concludes once all $k$ items on the beam are complete.\n",
        "\n",
        "With beam search, you should get an improvement of at least 0.5 BLEU over greedy search, and should reach above 21 BLEU without attention and above 37 BLEU with attention.\n",
        "\n",
        "**Tips:**\n",
        "\n",
        "1) A good general strategy when doing complex code like this is to carefully annotate each line with a comment saying what each dimension represents.\n",
        "\n",
        "2) You should only need one call to topk per step. You do not need to have a topk just over vocabulary first, you can directly go from vocab_size*beam_size to beam_size items.\n",
        "\n",
        "3) Be sure you are correctly keeping track of which beam item a candidate is selected from and updating the beam states, such as LSTM hidden state, accordingly. A single state from the previous time step may need to be used for multiple new beam items or not at all. This includes all state associated with a beam, including all past tokens output by the beam and any extra tensors such as ones remembering when a beam is finished.\n",
        "\n",
        "4) Pay attention to how you interleave things when using a single dimension to represent multiple things.  It will make a difference when you start reshaping to separate them out.  It may be easier to start with everything separate, then temporarily combine as needed.\n",
        "\n",
        "5) For efficiency, we suggest that you implement all beam manipulations using batched PyTorch computations rather than Python for-loops.\n",
        "\n",
        "6) Once an EOS token has been generated, force the output for that candidate to be padding tokens in all subsequent time steps by adding a large positive number like 1e9 to the appropriate logits. This will ensure that the candidate stays on the beam, as its probability will be very close to 1 and its score will effectively remain the same as when it was first completed.  All other (invalid) token continuations will have extremely low log probability and will not make it onto the beam.\n",
        "\n",
        "7) While you are encouraged to keep your tensor dimensions constant for simplicity (aside from the sequence length), some special care will need to be taken on the first iteration to ensure that your beam doesn't fill up with k identical copies of the same candidate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-11T17:18:08.323544Z",
          "iopub.status.busy": "2022-02-11T17:18:08.32329Z",
          "iopub.status.idle": "2022-02-11T17:18:11.003734Z",
          "shell.execute_reply": "2022-02-11T17:18:11.003018Z",
          "shell.execute_reply.started": "2022-02-11T17:18:08.323516Z"
        },
        "id": "-n3h47bJ8sz-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def predict_beam(model, sentences, k=5, max_length=100):\n",
        "  \"\"\"Make predictions for the given inputs using beam search.\n",
        "\n",
        "  Args:\n",
        "    model: A sequence-to-sequence model.\n",
        "    sentences: A list of input sentences, represented as strings.\n",
        "    k: The size of the beam.\n",
        "    max_length: The maximum length at which to truncate outputs in order to\n",
        "      avoid non-terminating inference.\n",
        "\n",
        "  Returns:\n",
        "    A list of beam predictions. Each element in the list should be a list of k\n",
        "    strings corresponding to the top k predictions for the corresponding input,\n",
        "    sorted in descending order by score.\n",
        "  \"\"\"\n",
        "\n",
        "  # Requirement: your implementation must be batched. This means that you should\n",
        "  # make only one call to model.encode() at the start of the function, and make\n",
        "  # only one call to model.decode() per inference step.\n",
        "\n",
        "  # Does top-k return relative ordering, if not how to return at end of method?\n",
        "  # EOS candidate getting knocked.\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  ...\n",
        "\n",
        "  # BEGIN SOLUTION\n",
        "\n",
        "  # END SOLUTION\n",
        "\n",
        "print(\"Baseline model validation BLEU using beam search:\",\n",
        "      evaluate(baseline_model, validation_data, method=\"beam\"))\n",
        "print()\n",
        "print(\"Baseline model sample predictions:\")\n",
        "print()\n",
        "show_predictions(baseline_model, include_beam=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1XCU1CNtupU"
      },
      "outputs": [],
      "source": [
        "print(\"Attention model validation BLEU using beam search:\",\n",
        "      evaluate(attention_model, validation_data, method=\"beam\"))\n",
        "print()\n",
        "print(\"Attention model sample predictions:\")\n",
        "print()\n",
        "show_predictions(attention_model, include_beam=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}